import os
import json
import hashlib
from collections import defaultdict
from datetime import datetime

# –ü—É—Ç–∏
target_dir = "C:\–û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ–º–µ–π–Ω—ã–π –∞—Ä—Ö–∏–≤"
hash_cache_path = os.path.join(target_dir, "_checked_hashes.json")


def get_file_hash(file_path):
    """–í—ã—á–∏—Å–ª—è–µ—Ç MD5-—Ö–µ—à —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    try:
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
        return None


def check_new_files(target_dir, hashes):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö –ø—É—Ç–µ–π."""
    duplicates = []
    trash_dir = os.path.join(target_dir, "_trash")

    for root, _, files in os.walk(target_dir):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞–ø–∫—É _trash
        if "_trash" in root.split(os.sep):
            continue

        for file in files:
            try:
                file_path = os.path.join(root, file)

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø—É—Ç–µ–π –¥–ª—è Windows
                if len(file_path) > 200:
                    file_path = "\\\\?\\" + os.path.abspath(file_path)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
                if file in ("_checked_hashes.json", "_duplicates_report.json"):
                    continue

                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏—è
                mod_time = os.path.getmtime(file_path)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
                if file_path in hashes:
                    cached_data = hashes[file_path]
                    if isinstance(cached_data, list):
                        cached_time, cached_hash = cached_data
                    else:
                        cached_time, cached_hash = mod_time, cached_data

                    if mod_time <= cached_time:
                        continue

                # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à
                file_hash = get_file_hash(file_path)
                if not file_hash:
                    continue

                # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                is_duplicate = False
                for cached_file, cached_data in hashes.items():
                    if isinstance(cached_data, list):
                        _, cached_hash = cached_data
                    else:
                        cached_hash = cached_data

                    if cached_hash == file_hash and cached_file != file_path:
                        duplicates.append({
                            "original": cached_file,
                            "duplicate": file_path
                        })
                        is_duplicate = True
                        break

                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
                hashes[file_path] = [mod_time, file_hash]

                if not is_duplicate:
                    print(f"üü¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã–π: {file_path[:100]}...")
                else:
                    print(f"üî¥ –î—É–±–ª—å: {file_path[:100]}...")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file[:50]}...: {str(e)}")
                continue

    return duplicates


def find_duplicates_from_cache(target_dir):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ _checked_hashes.json.
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç _duplicates_report.json –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
    [{"original": "path1", "duplicate": "path2"}, ...]
    """
    cache_path = os.path.join(target_dir, "_checked_hashes.json")
    report_path = os.path.join(target_dir, "_duplicates_report.json")

    if not os.path.exists(cache_path):
        print("–û—à–∏–±–∫–∞: —Ñ–∞–π–ª _checked_hashes.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É (–∫–æ–º–∞–Ω–¥–∞ 1).")
        return []

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–µ—à
    with open(cache_path, 'r', encoding='utf-8') as f:
        checked_hashes = json.load(f)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ö–µ—à–∞–º
    hash_groups = defaultdict(list)
    for file_path, (_, file_hash) in checked_hashes.items():
        hash_groups[file_hash].append(file_path)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥—É–±–ª–µ–π (—Ñ–∞–π–ª—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ö–µ—à–∞–º–∏)
    duplicates = []
    for files in hash_groups.values():
        if len(files) > 1:
            # –ü–µ—Ä–≤—ã–π —Ñ–∞–π–ª –≤ –≥—Ä—É–ø–ø–µ —Å—á–∏—Ç–∞–µ–º "–æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º"
            original = files[0]
            for duplicate in files[1:]:
                duplicates.append({"original": original, "duplicate": duplicate})

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(duplicates, f, indent=4, ensure_ascii=False)

    print(f"–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}. –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")
    return duplicates


def find_duplicates(target_dir):
    """–ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã"""
    hashes = defaultdict(list)

    for root, _, files in os.walk(target_dir):
        for file in files:
            file_path = os.path.join(root, file)
            file_hash = get_file_hash(file_path)
            if file_hash:
                hashes[file_hash].append(file_path)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = []
    for hash_val, files in hashes.items():
        if len(files) > 1:
            for i in range(1, len(files)):
                duplicates.append({
                    "original": files[0],
                    "duplicate": files[i]
                })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open("_duplicates_report.json", 'w') as f:
        json.dump(duplicates, f, indent=4)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    return duplicates

def load_checked_hashes():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–µ—à —Ö–µ—à–µ–π –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π."""
    if os.path.exists(hash_cache_path):
        with open(hash_cache_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_checked_hashes(hashes):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–µ—à —Ö–µ—à–µ–π –≤ —Ñ–∞–π–ª."""
    with open(hash_cache_path, 'w', encoding='utf-8') as f:
        json.dump(hashes, f, indent=4, ensure_ascii=False)


# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç –æ –¥—É–±–ª—è—Ö
def save_report_of_doblicates(duplicates):
    if duplicates:
        report_path = os.path.join(target_dir, "_duplicates_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(duplicates, f, indent=4, ensure_ascii=False)
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–µ–π: {len(duplicates)}. –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")
    else:
        print("\n–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")